{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lottery Ticket Hypothesis\n",
    "##### Due to Jonathan Frankle and Michael Carbin (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a feedforward neural network made to recognize handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some setup:\n",
    "from digit import *\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "# Define the transformations for the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a Net class with init() and forward() methods:\n",
    "We expect 28x28 pixel images to be mapped to 10 outputs, the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing new so far. Let's initialize a network, and for a secret reason, save the initial state. Now we can train and test the net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.460\n",
      "[2,  1000] loss: 0.162\n",
      "[3,  1000] loss: 0.121\n",
      "[4,  1000] loss: 0.096\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "torch.save(net.state_dict(), 'init.pt')\n",
    "\n",
    "train_network(net, trainloader, None, 4)\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! The network correctly identifies over 90% of the test images in just four epochs. \n",
    "\n",
    "\n",
    "## Pruning\n",
    "\n",
    "You may have heard of the concept of \"pruning\" a neural network. In general, pruning is the removal of select weights or neurons from a network to save on space and computation. For our purposes, just think of it as the zeroing-out of some percentage of the __lowest weights__ in a network. In their 1989 paper _\"Optimal Brain Damage,\"_ Yann Lecun, John Denker, and Sara Solla claim that \"By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples, and improved speed of learning and/or classification.\"\n",
    "\n",
    "Let's give our own little net some brain damage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 40140 weights to prune.\n",
      "Pruning layer fc2.weight with 3276 weights to prune.\n",
      "Pruning layer fc3.weight with 256 weights to prune.\n",
      "Sparsity in model: 40.00%\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "# set bottom 40% of weights to 0\n",
    "pruning_percent = 0.4\n",
    "p40_pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Cutting 40% of the weights seems not to have hurt our accuracy. How far can this idea be pushed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 60211 weights to prune.\n",
      "Pruning layer fc2.weight with 4915 weights to prune.\n",
      "Pruning layer fc3.weight with 384 weights to prune.\n",
      "Sparsity in model: 60.00%\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "# set bottom 60% of weights to 0\n",
    "pruning_percent = 0.6\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 80281 weights to prune.\n",
      "Pruning layer fc2.weight with 6553 weights to prune.\n",
      "Pruning layer fc3.weight with 512 weights to prune.\n",
      "Sparsity in model: 80.00%\n",
      "Accuracy of the network on the 10000 test images: 85 %\n"
     ]
    }
   ],
   "source": [
    "# set bottom 80% of weights to 0\n",
    "pruning_percent = 0.8\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it's starting to suffer a little... One more go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 95334 weights to prune.\n",
      "Pruning layer fc2.weight with 7782 weights to prune.\n",
      "Pruning layer fc3.weight with 608 weights to prune.\n",
      "Sparsity in model: 95.00%\n",
      "Accuracy of the network on the 10000 test images: 21 %\n"
     ]
    }
   ],
   "source": [
    "# set bottom 95% of weights to 0\n",
    "pruning_percent = 0.95\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, our experiment with pruning has come to an end. Its practical limit (in this case) seems to have been between 60% and 80%, which is impressive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.makeameme.org/created/not-so-fast-e35e21f418.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://media.makeameme.org/created/not-so-fast-e35e21f418.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just pruning the network, let's retrain the 40% pruned version with the original initialization weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.473\n",
      "[2,  1000] loss: 0.146\n",
      "[3,  1000] loss: 0.108\n",
      "[4,  1000] loss: 0.087\n",
      "Sparsity in model: 40.00%\n",
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Reset\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('init.pt'))\n",
    "\n",
    "# Train with 40% removed\n",
    "train_network(net, trainloader, p40_pruning_mask, 4)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another 40% off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 64225 weights to prune.\n",
      "Pruning layer fc2.weight with 5242 weights to prune.\n",
      "Pruning layer fc3.weight with 409 weights to prune.\n",
      "[1,  1000] loss: 0.533\n",
      "[2,  1000] loss: 0.157\n",
      "[3,  1000] loss: 0.114\n",
      "[4,  1000] loss: 0.100\n",
      "Sparsity in model: 64.00%\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "pruning_percent = 1 - 0.6**2\n",
    "\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "# Reset\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('init.pt'))\n",
    "\n",
    "# Train with 64% removed\n",
    "train_network(net, trainloader, pruning_mask, 4)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 78675 weights to prune.\n",
      "Pruning layer fc2.weight with 6422 weights to prune.\n",
      "Pruning layer fc3.weight with 501 weights to prune.\n",
      "[1,  1000] loss: 0.596\n",
      "[2,  1000] loss: 0.190\n",
      "[3,  1000] loss: 0.138\n",
      "[4,  1000] loss: 0.114\n",
      "Sparsity in model: 78.40%\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "pruning_percent = 1 - 0.6**3\n",
    "\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "# Reset\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('init.pt'))\n",
    "\n",
    "# Train with 78.4% removed\n",
    "train_network(net, trainloader, pruning_mask, 4)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to 95%!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer fc1.weight with 95669 weights to prune.\n",
      "Pruning layer fc2.weight with 7809 weights to prune.\n",
      "Pruning layer fc3.weight with 610 weights to prune.\n",
      "[1,  1000] loss: 1.028\n",
      "[2,  1000] loss: 0.227\n",
      "[3,  1000] loss: 0.177\n",
      "[4,  1000] loss: 0.155\n",
      "Sparsity in model: 95.33%\n",
      "Accuracy of the network on the 10000 test images: 95 %\n"
     ]
    }
   ],
   "source": [
    "pruning_percent = 1 - 0.6**6 \n",
    "\n",
    "pruning_mask, net = prune_network(net, pruning_percent)\n",
    "\n",
    "# Reset\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('init.pt'))\n",
    "\n",
    "# Train with 95.33% removed\n",
    "train_network(net, trainloader, pruning_mask, 4)\n",
    "\n",
    "print(\"Sparsity in model: {:.2f}%\".format(100 * float(torch.sum(net.fc1.weight == 0) + torch.sum(net.fc2.weight == 0) + torch.sum(net.fc3.weight == 0)) / float(net.fc1.weight.nelement() + net.fc2.weight.nelement() + net.fc3.weight.nelement())))\n",
    "test_network(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lottery Ticket Hypothesis\n",
    "\n",
    "... is the idea that when a dense neural network is successfully trained, it is successful by virtue of having found a subnetwork which performs the necessary operations. This subnetwork is usually a fraction of the size the overall dense neural network, and if the wieghts outside of the subnetwork are removed, the subnetwork can still perform quite well.\n",
    "\n",
    "This **sparse** subnetwork (with 5-20% of the weights of the full network) can be formed out of the initialization weights and trained more quickly, sometimes to a higher degree of accuracy on the test data, than the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Falberton.info%2Fimages%2Farticles%2Fgraphs%2Fgraphs_complete_sparse.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = \"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Falberton.info%2Fimages%2Farticles%2Fgraphs%2Fgraphs_complete_sparse.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization matters\n",
    "When the same sparse networks are used but new initialization weights are chosen, the models take much longer to train. This is because those initialization weights are part of what determine the structure of the subnetwork; they're poised to get to where they need to go in response to feedback from the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's a catch!\n",
    "\n",
    "PyTorch actually doesn't have support for spare neural nets, so we have to use dense tensors with zeros everywhere a weight is missing. This actually takes *the same amount of time and energy* to train and run as a full dense network, so ... while **in theory** the lottery ticket hypothesis implies better time and energy efficiency for neural networks, the state-of-the-art tools for building neural networks haven't yet implemented tools for taking advantage of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://slj.ma/step.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = \"https://slj.ma/step.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://slj.ma/acc.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url = \"https://slj.ma/acc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original number of weights in the neural network was __109,184__, and the number of weights in the smallest winning ticket was __1965__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
